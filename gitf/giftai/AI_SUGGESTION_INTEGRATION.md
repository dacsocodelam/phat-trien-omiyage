# ğŸ¤– AI Suggestion Feature - Backend Integration Guide

## Overview

TÃ­nh nÄƒng "AI Suggestion" trong CardCreator.tsx cho phÃ©p ngÆ°á»i dÃ¹ng tá»± Ä‘á»™ng táº¡o lá»i chÃºc báº±ng AI dá»±a trÃªn tone, receiver, relationship vÃ  occasion.

## Frontend Implementation âœ…

### Component: `CardCreator.tsx`

#### New Props

```typescript
interface CardCreatorProps {
  // ... existing props
  onAiThinkingChange?: (isThinking: boolean) => void; // Callback Ä‘á»ƒ control 3D sphere
}
```

#### New States

```typescript
const [isAiThinking, setIsAiThinking] = useState(false);
const [aiSuggestions, setAiSuggestions] = useState<AISuggestion[]>([]);
const [aiError, setAiError] = useState("");
const [receiverName, setReceiverName] = useState("");
const [receiverInfo, setReceiverInfo] = useState("");
```

#### API Call Function

```typescript
fetchAiSuggestions(); // POST /api/ai/suggestions
```

---

## Backend Requirements ğŸ”§

### Rails API Endpoint

**Endpoint:** `POST /api/ai/suggestions`

**Headers:**

```
Content-Type: application/json
```

**Request Body:**

```json
{
  "tone": "emotional" | "funny" | "formal",
  "receiver": "å¤§åˆ‡ãªäºº",
  "relationship": "å‹äºº",
  "occasion": "ãŠç¥ã„"
}
```

**Success Response (200 OK):**

```json
{
  "suggestions": [
    {
      "id": "suggestion-1",
      "message": "ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã€å¿ƒã‹ã‚‰æ„Ÿè¬ğŸ’–",
      "confidence": 0.95
    },
    {
      "id": "suggestion-2",
      "message": "ã‚ãªãŸã«å‡ºä¼šãˆã¦å¹¸ã›ã§ã™âœ¨",
      "confidence": 0.9
    },
    {
      "id": "suggestion-3",
      "message": "ç‰¹åˆ¥ãªã‚ãªãŸã¸ã€æ„›ã‚’è¾¼ã‚ã¦ğŸŒ¸",
      "confidence": 0.85
    }
  ]
}
```

**Error Response (500 Internal Server Error):**

```json
{
  "error": "AI service unavailable"
}
```

---

## Rails Implementation Example

### 1. Create Controller

**File:** `backend/app/controllers/api/ai_controller.rb`

```ruby
module Api
  class AiController < ApplicationController
    # POST /api/ai/suggestions
    def suggestions
      tone = params[:tone] # "emotional", "funny", "formal"
      receiver = params[:receiver] || "å¤§åˆ‡ãªäºº"
      relationship = params[:relationship] || "å‹äºº"
      occasion = params[:occasion] || "ãŠç¥ã„"

      # Call AI service (OpenAI, Claude, or custom model)
      suggestions = AiMessageService.generate_suggestions(
        tone: tone,
        receiver: receiver,
        relationship: relationship,
        occasion: occasion
      )

      render json: {
        suggestions: suggestions.map.with_index do |message, index|
          {
            id: "suggestion-#{index + 1}",
            message: message,
            confidence: calculate_confidence(message, tone)
          }
        end
      }
    rescue StandardError => e
      Rails.logger.error("AI Suggestion Error: #{e.message}")
      render json: { error: "AI service unavailable" }, status: 500
    end

    private

    def calculate_confidence(message, tone)
      # Simple confidence calculation based on message length and tone
      base_confidence = 0.8
      length_bonus = [message.length / 100.0, 0.15].min
      base_confidence + length_bonus
    end
  end
end
```

### 2. Add Route

**File:** `backend/config/routes.rb`

```ruby
Rails.application.routes.draw do
  namespace :api do
    post 'ai/suggestions', to: 'ai#suggestions'
  end
end
```

### 3. Create AI Service

**File:** `backend/app/services/ai_message_service.rb`

```ruby
class AiMessageService
  # Using OpenAI GPT-4 as example
  def self.generate_suggestions(tone:, receiver:, relationship:, occasion:)
    # Initialize OpenAI client
    client = OpenAI::Client.new(access_token: ENV['OPENAI_API_KEY'])

    # Build prompt based on tone
    prompt = build_prompt(tone, receiver, relationship, occasion)

    # Call OpenAI API
    response = client.chat(
      parameters: {
        model: "gpt-4",
        messages: [
          { role: "system", content: "ã‚ãªãŸã¯æ—¥æœ¬èªã§ã‚®ãƒ•ãƒˆã‚«ãƒ¼ãƒ‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›¸ãå°‚é–€å®¶ã§ã™ã€‚" },
          { role: "user", content: prompt }
        ],
        temperature: 0.7,
        max_tokens: 200
      }
    )

    # Parse response and extract 3 messages
    messages = parse_ai_response(response)
    messages.take(3)
  rescue => e
    Rails.logger.error("OpenAI Error: #{e.message}")
    # Return fallback messages
    fallback_messages(tone)
  end

  def self.build_prompt(tone, receiver, relationship, occasion)
    tone_descriptions = {
      "emotional" => "æ„Ÿå‹•çš„ã§å¿ƒæ¸©ã¾ã‚‹",
      "funny" => "ãƒ¦ãƒ¼ãƒ¢ã‚¢ãŒã‚ã£ã¦æ¥½ã—ã„",
      "formal" => "ä¸å¯§ã§ç¤¼å„€æ­£ã—ã„"
    }

    <<~PROMPT
      #{receiver}ã•ã‚“ï¼ˆ#{relationship}ï¼‰ã¸ã®#{occasion}ã®ã‚®ãƒ•ãƒˆã‚«ãƒ¼ãƒ‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã€
      #{tone_descriptions[tone]}ãƒˆãƒ¼ãƒ³ã§3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚

      æ¡ä»¶:
      - å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯1è¡Œã€30-50æ–‡å­—ç¨‹åº¦
      - çµµæ–‡å­—ã‚’1-2å€‹å«ã‚ã‚‹
      - å¿ƒã«éŸ¿ãè¨€è‘‰ã‚’é¸ã¶

      ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ”¹è¡Œã§åŒºåˆ‡ã£ã¦å‡ºåŠ›
    PROMPT
  end

  def self.parse_ai_response(response)
    content = response.dig("choices", 0, "message", "content")
    content.split("\n").map(&:strip).reject(&:empty?)
  end

  def self.fallback_messages(tone)
    fallbacks = {
      "emotional" => [
        "ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã€å¿ƒã‹ã‚‰æ„Ÿè¬ğŸ’–",
        "ã‚ãªãŸã«å‡ºä¼šãˆã¦å¹¸ã›ã§ã™âœ¨",
        "ç‰¹åˆ¥ãªã‚ãªãŸã¸ã€æ„›ã‚’è¾¼ã‚ã¦ğŸŒ¸"
      ],
      "funny" => [
        "ã¾ãŸä¸€ã¤æ­³ã¨ã£ãŸã­ğŸ˜‚ğŸ‚",
        "ã„ã¤ã‚‚ç¬‘ã‚ã›ã¦ãã‚Œã¦ã‚µãƒ³ã‚­ãƒ¥ãƒ¼ğŸ¤£",
        "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚ˆã‚Šç§ãŒæœ€é«˜ã®ã‚®ãƒ•ãƒˆï¼ğŸ˜"
      ],
      "formal" => [
        "å¿ƒã‚ˆã‚ŠãŠç¥ã„ç”³ã—ä¸Šã’ã¾ã™ğŸŠ",
        "ã”å¥å‹ã‚’ãŠç¥ˆã‚Šã„ãŸã—ã¾ã™ğŸ™",
        "æ—¥é ƒã®æ„Ÿè¬ã‚’è¾¼ã‚ã¦è´ˆã‚Šã¾ã™âœ¨"
      ]
    }
    fallbacks[tone] || fallbacks["emotional"]
  end
end
```

### 4. Add Gem (if using OpenAI)

**File:** `backend/Gemfile`

```ruby
gem 'ruby-openai'
```

Then run:

```bash
bundle install
```

### 5. Environment Variables

**File:** `backend/.env`

```bash
OPENAI_API_KEY=sk-your-api-key-here
NEXT_PUBLIC_BACKEND_URL=http://localhost:3001
```

---

## Frontend Environment Variables

**File:** `frontend/.env.local`

```bash
NEXT_PUBLIC_BACKEND_URL=http://localhost:3001
# Or for production with Ngrok:
# NEXT_PUBLIC_BACKEND_URL=https://your-ngrok-url.ngrok.io
```

---

## Testing

### Manual Test

1. Start Rails backend:

```bash
cd backend
rails server -p 3001
```

2. Start Next.js frontend:

```bash
cd frontend
npm run dev
```

3. Open browser and test:
   - Select a tone (æ„Ÿå‹•/ãƒ¦ãƒ¼ãƒ¢ã‚¢/ãƒ•ã‚©ãƒ¼ãƒãƒ«)
   - Enter receiver name and relationship (optional)
   - Click "AI ã«ææ¡ˆã—ã¦ã‚‚ã‚‰ã†"
   - Watch the 3D sphere glow (if `onAiThinkingChange` callback is implemented)
   - See 3 AI-generated suggestions appear with glassmorphism effect
   - Click on a suggestion to see typing animation

### API Test with curl

```bash
curl -X POST http://localhost:3001/api/ai/suggestions \
  -H "Content-Type: application/json" \
  -d '{
    "tone": "emotional",
    "receiver": "ç¾å’²",
    "relationship": "è¦ªå‹",
    "occasion": "èª•ç”Ÿæ—¥"
  }'
```

Expected response:

```json
{
  "suggestions": [
    {
      "id": "suggestion-1",
      "message": "ç¾å’²ã¸ã€ã„ã¤ã‚‚ãã°ã«ã„ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ğŸ’–",
      "confidence": 0.95
    },
    ...
  ]
}
```

---

## UI Features âœ¨

### 1. **Glow Button**

- Button phÃ¡t sÃ¡ng vá»›i accent color theo tone Ä‘Ã£ chá»n
- Animated pulse effect khi loading

### 2. **3D Sphere Integration**

- Callback `onAiThinkingChange(true)` trigger sphere animation
- Sphere cÃ³ thá»ƒ glow máº¡nh hÆ¡n, xoay nhanh hÆ¡n khi AI Ä‘ang suy nghÄ©

### 3. **Glassmorphism Mini Cards**

- Suggestions hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng glass cards
- `backdrop-blur-md`, `bg-white/10`, border má»

### 4. **Stagger Animation**

- Framer Motion staggerChildren
- Má»—i card xuáº¥t hiá»‡n láº§n lÆ°á»£t vá»›i delay 0.1s

### 5. **Typing Effect**

- Khi click suggestion, text "cháº£y" vÃ o textarea
- Sá»­ dá»¥ng `Array.from()` Ä‘á»ƒ handle emojis Ä‘Ãºng

### 6. **Confidence Bar**

- Progress bar hiá»ƒn thá»‹ confidence score
- Animated width vá»›i gradient color

---

## Error Handling

Frontend tá»± Ä‘á»™ng handle cÃ¡c lá»—i:

- **Timeout (15s):** â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
- **500 Error:** ğŸ”§ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
- **429 Rate Limit:** âš ï¸ ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™
- **Network Error:** âŒ AIç”Ÿæˆã‚¨ãƒ©ãƒ¼

Khi cÃ³ lá»—i, fallback messages sáº½ Ä‘Æ°á»£c hiá»ƒn thá»‹.

---

## Performance Optimization

1. **Request Timeout:** 15 seconds
2. **Debounce:** User input debounced (if needed)
3. **Cache:** Consider caching suggestions for same params
4. **Rate Limiting:** Implement rate limiting on backend

---

## Future Enhancements

- [ ] Add language selection (English, Vietnamese, etc.)
- [ ] Save favorite suggestions to database
- [ ] User feedback on suggestions (thumbs up/down)
- [ ] Custom AI model fine-tuned on Japanese gift messages
- [ ] Real-time streaming response (SSE or WebSocket)

---

## Support

For issues or questions, contact the development team.

**Last Updated:** January 19, 2026
